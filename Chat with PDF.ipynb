{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e511b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example PDF file path\n",
    "pdf_path = \"tables-charts-and-graphs-with-examples-from.pdf\"  # Replace with your PDF file path\n",
    "pdf_content = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77df5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to create chunks and embeddings\n",
    "def create_chunks_and_embeddings(content, chunk_size=512):\n",
    "    # Split content into chunks\n",
    "    chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    return chunks, embeddings\n",
    "\n",
    "# Create chunks and embeddings\n",
    "chunks, embeddings = create_chunks_and_embeddings(pdf_content)\n",
    "\n",
    "# Convert to numpy array for FAISS\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)  # Add embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e218c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(query):\n",
    "    # Convert the query to an embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Perform a similarity search\n",
    "    D, I = index.search(np.array(query_embedding).astype('float32'), k=5)  # Retrieve top 5 results\n",
    "    return I[0]  # Return indices of the most relevant chunks\n",
    "\n",
    "# Example query\n",
    "user_query = \"What is the unemployment information based on type of degree?\"\n",
    "relevant_indices = handle_query(user_query)\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "relevant_chunks = [chunks[i] for i in relevant_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7193bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the following information, answer the question: What is the unemployment information based on type of degree?\n",
      "\n",
      "Context:  Bureau of Labor Statistics\n",
      "19%\n",
      "18%\n",
      "4%\n",
      "59%\n",
      "2015 U.S. GDP (in millions of dollars)\n",
      "Manufacturing\n",
      "Finance, insurance, real\n",
      "estate, rental, and\n",
      "leasing\n",
      "Arts, entertainment,\n",
      "recreation,\n",
      "accommodation, and\n",
      "food services\n",
      "Other\n",
      "• The chart below is called a pie chart.  It shows what \n",
      "percent “of the pie” a particular category occupies \n",
      "out of the whole.\n",
      "• If total GDP in 2015 is the entire pie, then \n",
      "manufacturing makes up 19% of that pie and finance \n",
      "makes up 18%.  Notice that visually speaking, since 19% \n",
      "and 18 ng different groups of \n",
      "variables.  We used it to compare different components \n",
      "of US GDP.  We did the same with the pie chart; \n",
      "depending on your purposes you may choose to use a \n",
      "pie chart or a bar graph.\n",
      "x\n",
      "y\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "9\n",
      "4\n",
      "12\n",
      "5\n",
      "15\n",
      "6\n",
      "18\n",
      "7\n",
      "21\n",
      "8\n",
      "24\n",
      "•\n",
      "If given a table of data, we should be able to plot it.  Below is \n",
      "some sample data; plot the data with x on the x-axis and y on the \n",
      "y-axis.\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "•\n",
      "Below is a plot of the data on the table from the previous \n",
      "slide.  Notice th le of Yearly U.S. GDP by \n",
      "Industry (in millions of dollars)\n",
      "Year\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "All Industries\n",
      "26093515\n",
      "27535971\n",
      "28663246\n",
      "29601191\n",
      "30895407\n",
      "31397023\n",
      "Manufacturing\n",
      "4992521\n",
      "5581942\n",
      "5841608\n",
      "5953299\n",
      "6047477\n",
      "5829554\n",
      "Finance,\n",
      "Insurance, Real \n",
      "Estate, Rental, \n",
      "Leasing\n",
      "4522451\n",
      "4618678\n",
      "4797313\n",
      "5031881\n",
      "5339678\n",
      "5597018\n",
      "Arts, \n",
      "Entertainment, \n",
      "Recreation, \n",
      "Accommodation,\n",
      "and Food Service\n",
      "964032\n",
      "1015238\n",
      "1076249\n",
      "1120496\n",
      "1189646\n",
      "1283813\n",
      "Other\n",
      "15614511\n",
      "16320113\n",
      "16948076\n",
      "17495515\n",
      "18318606\n",
      "18686638\n",
      "Source: U.S. tory\n",
      "In what years were the affiliations for \n",
      "Republicans and Independents the \n",
      "same?\n",
      "During what time period did the \n",
      "party affiliations have the most \n",
      "change?\n",
      "Example from Education\n",
      "What percent of the total \n",
      "class received grades of 72 or \n",
      "77?\n",
      "Which grade showed the \n",
      "largest difference between \n",
      "males and females?\n",
      "Example from Psychology\n",
      "What do you notice \n",
      "is different in this \n",
      "graph than the \n",
      "others reviewed so \n",
      "far?\n",
      "  called a bar graph.  \n",
      "•\n",
      "It shows each of the variables independent of each other, each \n",
      "with its own bar.\n",
      "•\n",
      "2015 GDP for all industries was $31.397023; looking at the graph, \n",
      "the bar for all industries is just above $30.\n",
      "•\n",
      "One is still be able compare each variable with the other by \n",
      "comparing bars.\n",
      "•\n",
      "The graph below is called a line graph.  It shows how a variable \n",
      "evolves with respect to another variable.  In the line graph below, we \n",
      "show how GDP has evolved by year.\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "1947\n",
      "1950\n",
      "1953\n",
      "\n",
      "Answer:\n",
      "0\n",
      "11\n",
      "6\n",
      "13\n",
      "18\n",
      "\n",
      "37\n",
      "\n",
      "33\n",
      "\n",
      "29\n",
      "\n",
      "24\n",
      "\n",
      "33\n",
      "\n",
      "27\n",
      "\n",
      "26\n",
      "\n",
      "20\n",
      "\n",
      "16\n",
      "\n",
      "11\n",
      "\n",
      "12\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained language model for response generation\n",
    "llm = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def generate_response(relevant_chunks, user_query):\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    prompt = f\"Based on the following information, answer the question: {user_query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Use max_new_tokens instead of max_length\n",
    "    response = llm(prompt, max_new_tokens=50, num_return_sequences=1)  # Generate up to 50 new tokens\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Generate a response\n",
    "response = generate_response(relevant_chunks, user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9468b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
